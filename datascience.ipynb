{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgmXRT9dlm9p6FGrwOWozt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHDS-2002/Data-Science-Technologies/blob/main/datascience.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Технологии Data Science**"
      ],
      "metadata": {
        "id": "TxW3QaAdIvbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Нейронная сеть**"
      ],
      "metadata": {
        "id": "jdmILrdmJBt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нейронная сеть - математическая модель, математическая функция, имитирующая процессы, протекающие в мозге любого живого организма.\n",
        "\n",
        "Имеет огромное прикладное применение в науке, бизнесе, IT, медицине, экономике, различных промышленных отраслях.\n",
        "\n",
        "Имеет достаточно примитивное строение: входной слой, скрытые слои, выходной слой.\n",
        "\n",
        "Обучается по методу обучения с учителем: принимает данные, обрабатывает при помощи весов нейронов, выдаёт выходные данные, происходит суммирование квадратов ошибок - разностей между ожидаемым результатом и полученым; происходит корректировка весов нейронов методом обратного распространения.\n",
        "\n",
        "Основные параметры нейронной сети - веса.\n",
        "Веса каждого слоя нейронной сети представляются в виде матриц - математических объектов прямоугольной формы, состоящей из m строк и n столбцов. Могут представляться в виде тензоров."
      ],
      "metadata": {
        "id": "JUP1Kzq1JOzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Обучение нейронной сети**"
      ],
      "metadata": {
        "id": "ewLtn8WekFPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Обучение нейронной сети можно произвести при помощи алгоритмов глубокого обучения: упругое распространение, обратное распространения, градиентный спуск, генетические алгоритмы."
      ],
      "metadata": {
        "id": "Pq7FVO2ygvb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Задачи классификации**"
      ],
      "metadata": {
        "id": "81LPDr_mgZRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DQ40mWBl27Pg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Решение задач классификации при помощи нейронной сети**"
      ],
      "metadata": {
        "id": "qcVoHoWelJPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u07nSjK_29Q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Задачи регрессии**"
      ],
      "metadata": {
        "id": "tZU2KXDRggbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_UqI1AMZ29wu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Решение задач регрессии при помощи нейронной сети**"
      ],
      "metadata": {
        "id": "a8Xn3Vuck_ei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TFOaw_e_2-Mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Применение библиотек глубокого и машинного обучения в решении задач классификации и регрессии**"
      ],
      "metadata": {
        "id": "2v_Ug2KOlqmX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2w9lveviHS7"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "import torch as tor\n",
        "import sklearn as sk\n",
        "from time import time\n",
        "import torch.nn as nn\n",
        "import tensorflow as tf\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import transforms\n",
        "from tensorflow.keras.preprocessing import image"
      ],
      "metadata": {
        "id": "8sjNNewGjlQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Решение задачи классификации при помощи PyTorch**"
      ],
      "metadata": {
        "id": "q77lqBPRehAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "#                                 #\n",
        "#             PyTorch             #\n",
        "#                                 #\n",
        "###################################\n",
        "\n",
        "###################################\n",
        "#         Classification          #\n",
        "###################################\n",
        "\n",
        "# Prepare the dataset for the classification\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform)\n",
        "trainloader = tor.utils.data.DataLoader(trainset, batch_size=4, shuffle=True,\n",
        "                                        num_workers=2)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = tor.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                       shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
        "           'ship', 'truck')\n",
        "\n",
        "#Define the CNN architecture\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(-1, 16 * 5 * 5)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "net = Net()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for i, data in enumerate(trainloader, 0):\n",
        "    inputs, labels = data\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    if i % 2000 == 1999: # print every 2000 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
        "      running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Test the network\n",
        "correct = total = 0\n",
        "\n",
        "with tor.no_grad():\n",
        "  for data in testloader:\n",
        "    images, labels = data\n",
        "    outputs = net(images)\n",
        "    _, predicted = tor.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n",
        "\n",
        "###################################\n",
        "#         Classification          #\n",
        "###################################\n",
        "\n",
        "###################################\n",
        "#                                 #\n",
        "#             PyTorch             #\n",
        "#                                 #\n",
        "###################################"
      ],
      "metadata": {
        "id": "EHxCORDgh8fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bioinfokit"
      ],
      "metadata": {
        "id": "r9rUm5-ulZUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Решение задачи регрессии при помощи PyTorch**"
      ],
      "metadata": {
        "id": "W7t5JM4omkMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "#                                 #\n",
        "#             PyTorch             #\n",
        "#                                 #\n",
        "###################################\n",
        "\n",
        "###################################\n",
        "#           Regression            #\n",
        "###################################\n",
        "\n",
        "from bioinfokit.visuz import stat\n",
        "from sklearn.metrics import r2_score\n",
        "from bioinfokit.analys import get_data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.family'] = 'Times New Roman'\n",
        "\n",
        "# Prepare the dataset for the regression\n",
        "ds = get_data('plant_richness_lr').data\n",
        "ds.head(10)\n",
        "\n",
        "# convert variables to PyTorch tensor\n",
        "x = tor.tensor(ds[['area']].values, dtype=tor.float32)\n",
        "y = tor.tensor(ds[['ntv_rich']].values, dtype=tor.float32)\n",
        "\n",
        "# build regression model\n",
        "\n",
        "in_features = 1 # number of independent variables\n",
        "out_features = 1 # dimension of predicted variables\n",
        "\n",
        "# bias is default true and can be skipped\n",
        "regression = tor.nn.Linear(in_features=in_features, out_features=out_features,\n",
        "                           bias=True)\n",
        "\n",
        "# define loss function\n",
        "mse_loss = tor.nn.MSELoss()\n",
        "\n",
        "# define gradient descent optimizer\n",
        "optimizer = tor.optim.SGD(regression.parameters(), lr=0.002)\n",
        "\n",
        "# set epoch to 6K\n",
        "epochs = 6000\n",
        "for i in range(epochs):\n",
        "  # predict model with current regression parameters\n",
        "  # forward pass (feed the data to model)\n",
        "  pred_y = regression(x)\n",
        "  # calculate loss function\n",
        "  step_loss = mse_loss(pred_y, y)\n",
        "\n",
        "  # Backward to find the derivatives of the loss function with respect to\n",
        "  # regression parameters\n",
        "  # make any stored gradients to zero\n",
        "  # backward pass (go back and update the regression parameters to minimize the\n",
        "  # loss)\n",
        "  optimizer.zero_grad()\n",
        "  step_loss.backward()\n",
        "  # update with current step regression parameters\n",
        "  optimizer.step()\n",
        "  print('epoch [{}], Loss: {:.2f}'.format(i, step_loss.item()))\n",
        "\n",
        "# estimate the regression parameters\n",
        "# bias b (offset or y-intercept)\n",
        "regression.bias.item()\n",
        "# weight (w)\n",
        "regression.weight.item()\n",
        "\n",
        "# detach will not build a gradient computational graph (no backpropagation)\n",
        "pred_y = regression(x).detach()\n",
        "ds['yhat'] = pred_y.numpy()\n",
        "stat.regplot(df=ds, x='area', y='ntv_rich', yhat='yhat')\n",
        "\n",
        "# model performance\n",
        "r2_score(y_true=y, y_pred=pred_y.detach().numpy())\n",
        "\n",
        "# prediction\n",
        "area = 3\n",
        "# predict y (ntv_rich) value when x(area) is 3\n",
        "pred_y = regression(tor.tensor([[area]], dtype=tor.float32)).detach()\n",
        "pred_y.item()\n",
        "\n",
        "###################################\n",
        "#           Regression            #\n",
        "###################################\n",
        "\n",
        "###################################\n",
        "#                                 #\n",
        "#             PyTorch             #\n",
        "#                                 #\n",
        "###################################"
      ],
      "metadata": {
        "id": "usXtd0iDi6el",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "4d540b9f-1185-4b75-885e-72d49fd4cd8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bioinfokit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m###################################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#                                 #\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#             PyTorch             #\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#           Regression            #\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m###################################\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbioinfokit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisuz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stat\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m r2_score\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbioinfokit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_data\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bioinfokit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://upload.wikimedia.org/wikipedia/commons/b/b5/Lion_d%27Afrique.jpg \\\n",
        "    -O /tmp/lion.jpg"
      ],
      "metadata": {
        "id": "YglUVY3tuvTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Решение задачи классификации при помощи TensorFlow**"
      ],
      "metadata": {
        "id": "qvwAhmQ5mwNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "#                                 #\n",
        "#           Tensorflow            #\n",
        "#                                 #\n",
        "###################################\n",
        "\n",
        "###################################\n",
        "#         Classification          #\n",
        "###################################\n",
        "\n",
        "# Prepare the dataset for the classification\n",
        "classification = tf.keras.applications.Xception(\n",
        "    include_top=True,\n",
        "    weights='imagenet',\n",
        "    input_tensor=None,\n",
        "    input_shape=None,\n",
        "    pooling=None,\n",
        "    classes=1000,\n",
        "    classifier_activation='softmax',\n",
        ")\n",
        "\n",
        "img_path = '/tmp/lion.jpg'\n",
        "img = tf.keras.preprocessing.image.load_img(img_path, target_size=(299, 299))\n",
        "x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = tf.keras.applications.xception.preprocess_input(x)\n",
        "\n",
        "preds = classification.predict(x)\n",
        "# decode the results into a list of tuples (class, description, probability)\n",
        "# (one such list for each sample in the batch)\n",
        "print('Predicted:', tf.keras.applications.xception.decode_predictions(preds, top=3)[0])\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "image = X_train[785]\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "\n",
        "# Data preprocessing\n",
        "\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# Classification\n",
        "classification = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32, 32, 5)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
        "\n",
        "        tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2,2), strides=2),\n",
        "\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(100, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "classification.summary()\n",
        "\n",
        "# plot_model\n",
        "tf.keras.utils.plot_model(\n",
        "    classification,\n",
        "    to_file='classfication.png',\n",
        "    show_shapes=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir='TB',\n",
        "    expand_nested=True,\n",
        "    dpi=96,\n",
        ")\n",
        "\n",
        "# compliling the model\n",
        "classification.compile(optimizer='SGD',\n",
        "                       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "# checkpoint\n",
        "checkpoint_filepath = '/tmp/checkpoint'\n",
        "'''classification_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    save_best_only=True\n",
        ")'''\n",
        "\n",
        "# callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=2),\n",
        "]\n",
        "\n",
        "#saved_model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "history = classification.fit(X_train, y_train, epochs=600,\n",
        "                             validation_data=(X_test, y_test),\n",
        "                             callbacks=callbacks)\n",
        "\n",
        "metrics_df = pd.DataFrame(history.history)\n",
        "metrics_df[['loss', 'val_loss']].plot()\n",
        "metrics_df[['accuracy', 'val_accuracy']].plot()\n",
        "\n",
        "# saving\n",
        "classification.save('classification.h5')\n",
        "\n",
        "# loaded\n",
        "load_saved_model = tf.keras.models.load_model('classification.h5')\n",
        "load_saved_model.summary()\n",
        "\n",
        "###################################\n",
        "#         Classification          #\n",
        "###################################\n",
        "\n",
        "###################################\n",
        "#                                 #\n",
        "#           Tensorflow            #\n",
        "#                                 #\n",
        "###################################"
      ],
      "metadata": {
        "id": "FyRNBQrFjEWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q seaborn"
      ],
      "metadata": {
        "id": "dTHlVgfja--C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Решение задачи регрессии при помощи TensorFlow**"
      ],
      "metadata": {
        "id": "CHp4SIn2m2TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "#                                 #\n",
        "#           Tensorflow            #\n",
        "#                                 #\n",
        "###################################\n",
        "\n",
        "###################################\n",
        "#           Regression            #\n",
        "###################################\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Make NumPy printouts easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "# Prepare the dataset for the regression\n",
        "\n",
        "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mog.data'\n",
        "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
        "                'Acceleration', 'Model Year', 'Origin']\n",
        "\n",
        "raw_dataset = pd.read_csv(url, names=column_names,\n",
        "                          na_values='?', comment='\\t',\n",
        "                          sep=' ', skipinitialspace=True)\n",
        "\n",
        "dataset = raw_dataset.copy()\n",
        "dataset.tail()\n",
        "\n",
        "dataset.isna().sum()\n",
        "dataset = dataset.dropna()\n",
        "\n",
        "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
        "dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\n",
        "dataset.tail()\n",
        "\n",
        "# The train the neural networks for the regression\n",
        "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)\n",
        "\n",
        "sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']],\n",
        "             diag_kind='kde')\n",
        "\n",
        "train_dataset.describe().transpose()\n",
        "\n",
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "train_labels = train_features.pop('MPG')\n",
        "test_labels = test_features.pop('MPG')\n",
        "\n",
        "# Normalization\n",
        "train_dataset.describe().transpose()[['mean', 'std']]\n",
        "\n",
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(np.array(train_features))\n",
        "\n",
        "print(normalizer.mean.numpy())\n",
        "\n",
        "first = np.array(train_features[:1])\n",
        "\n",
        "with np.printoptinos(precision=2, suppress=True):\n",
        "  print('First example:', first, '\\n')\n",
        "  print('Normalized:', normalizer(first).numpy())\n",
        "\n",
        "horsepower = np.array(train_features['Horsepower'])\n",
        "\n",
        "horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n",
        "horsepower_normalizer.adapt(horsepower)\n",
        "\n",
        "horsepower_model = tf.keras.Sequential([\n",
        "    horsepower_normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "horsepower_model.summary()\n",
        "\n",
        "horsepower_model.predict(horsepower[:10])\n",
        "\n",
        "horsepower_model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error'\n",
        ")\n",
        "\n",
        "%%time\n",
        "history = horsepower_model.fit(\n",
        "    train_features['Horsepower'],\n",
        "    train_labels,\n",
        "    epochs=100,\n",
        "    # Suppress logging.\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data.\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'].history.epoch\n",
        "hist.tail()\n",
        "\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.ylim([0, 10])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error [MPG]')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "plot_loss(history)\n",
        "\n",
        "test_results = {}\n",
        "\n",
        "test_results['horsepower_model'] = horsepower_model.evalueate(\n",
        "    test_features['Horsepower'],\n",
        "    test_labels, verbose=0\n",
        ")\n",
        "\n",
        "x = tf.linspace(0.0, 250, 251)\n",
        "y = horsepower_model.predict(x)\n",
        "\n",
        "def plot_horsepower(x, y):\n",
        "  plt.scatter(train_features['Horsepower'], train_labels, label='Data')\n",
        "  plt.plot(x, y, color='k', label='Predictions')\n",
        "  plt.xlabel('Horsepower')\n",
        "  plt.ylabel('MPG')\n",
        "  plt.legend()\n",
        "\n",
        "plot_horsepower(x, y)\n",
        "\n",
        "linear_model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "linear_model.predict(train_features[:10])\n",
        "linear_model.layers[1].kernel\n",
        "\n",
        "linear_model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error'\n",
        ")\n",
        "\n",
        "%%time\n",
        "history = linear_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    epochs=100,\n",
        "    # Suppress logging.\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data.\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "plot_loss(history)\n",
        "\n",
        "test_results['linear_model'] = linear_model.evaluate(\n",
        "    test_features, test_labels, verbose=0\n",
        ")\n",
        "\n",
        "def build_and_compile_model(norm):\n",
        "  model = keras.Sequential([\n",
        "      norm,\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  model.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
        "\n",
        "  return model\n",
        "\n",
        "dnn_horsepower_model = build_and_compile_model(horsepower_normalizer)\n",
        "dnn_horsepower_model.summary()\n",
        "\n",
        "%%time\n",
        "history = dnn_horsepower_model.fit(\n",
        "    train_features['Horsepower'],\n",
        "    train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=0, epochs=100\n",
        ")\n",
        "\n",
        "plot_loss(history)\n",
        "\n",
        "x = tf.linspace(0.0, 250, 251)\n",
        "y = dnn_horsepower_model.predict(x)\n",
        "\n",
        "plot_horsepower(x, y)\n",
        "\n",
        "test_results['dnn_horsepower_model'] = dnn_horsepower_model.evaluate(\n",
        "    test_features['Horsepower'], test_labels,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "dnn_model = build_and_compile_model(normalizer)\n",
        "dnn_model.summary()\n",
        "\n",
        "%%time\n",
        "history = dnn_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    validation_splot=0.2,\n",
        "    verbose=0, epochs=100\n",
        ")\n",
        "\n",
        "plot_loss(history)\n",
        "\n",
        "test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)\n",
        "\n",
        "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T\n",
        "\n",
        "test_predictions = dnn_model.predict(test_features).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "lims = [0, 50]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)\n",
        "\n",
        "error = test_predictions - test_labels\n",
        "plt.hist(error, bins=25)\n",
        "plt.xlabel('Prediction Error [MPG]')\n",
        "_ = plt.ylabel('Count')\n",
        "\n",
        "dnn_model.save('dnn_model')\n",
        "\n",
        "reloaded = tf.keras.models.load_model('dnn_model')\n",
        "\n",
        "test_results['reloaded'] = reloaded.evaluate(\n",
        "    test_features, test_labels, verbose=0\n",
        ")\n",
        "\n",
        "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T\n",
        "\n",
        "###################################\n",
        "#           Regression            #\n",
        "###################################\n",
        "\n",
        "###################################\n",
        "#                                 #\n",
        "#           Tensorflow            #\n",
        "#                                 #\n",
        "###################################"
      ],
      "metadata": {
        "id": "OohSTHqWjKxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qaBCcjBByd8",
        "outputId": "a5cfd736-78c0-48eb-8da0-52b6380e9b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Решение задачи классификации при помощи scikit-learn**"
      ],
      "metadata": {
        "id": "7eOqbhnvm_yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "#                                 #\n",
        "#          scikit-learn           #\n",
        "#                                 #\n",
        "###################################\n",
        "\n",
        "###################################\n",
        "#         Classification          #\n",
        "###################################\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "import keras.layers as l\n",
        "from keras import backend\n",
        "\n",
        "if 'tensorflow' == backend.backend():\n",
        "  import tensorflow as tf\n",
        "\n",
        "from keras.backend.tensorflow_backend import session\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.gpu_options.visible_device_list = '0'\n",
        "set_session(tf.Session(config=config))\n",
        "\n",
        "#Loading Data\n",
        "def get_token():\n",
        "  categories = None\n",
        "  remove = ('headers', 'footers', 'quotes')\n",
        "\n",
        "  data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
        "                                  shuffle=True, random_state=42,\n",
        "                                  remove=remove)\n",
        "  label_train = data_train.target\n",
        "  t = RegexpTokenizer(\"[\\w]+\")\n",
        "  content = []\n",
        "  for doc in data_train.data:\n",
        "    content.append(t.tokenize(doc))\n",
        "  print(len(content))\n",
        "  return {'data': content, 'label': label_train}\n",
        "\n",
        "def create_tokenizer(corpus):\n",
        "  t = Tokenizer()\n",
        "  t.fit_on_texts(corpus)\n",
        "  return t\n",
        "\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "  encoded = tokenizer.texts_to_sequences(docs)\n",
        "  padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "  return padded\n",
        "\n",
        "def cnn_model(X_train, Y_train, tokenizer, max_length):\n",
        "  model = Sequential()\n",
        "  model.add(l.InputLayer(input_shape=(max_length,), dtype='int32'))\n",
        "  model.add(l.Embedding(vocab_size, 100, input_length=max_length))\n",
        "  model.add(l.GRU(output_dim=Y_train.shape[-1], return_sequences=True))\n",
        "  model.add(l.MaxPooling1D(pool_size=2))\n",
        "  model.add(l.Flatten())\n",
        "  model.add(l.Dense(20, activation='softmax'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "def create_label(n):\n",
        "  label = list()\n",
        "  for i in range(0, 20):\n",
        "    if i != n:\n",
        "      label.append(0)\n",
        "    else:\n",
        "      label.append(1)\n",
        "\n",
        "  return label\n",
        "\n",
        "# Tokenizing data\n",
        "train_data = get_token()['data']\n",
        "label = get_token()['label']\n",
        "train_label = np.array([create_label(n) for n in label])\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.3)\n",
        "tokenizer = create_tokenizer(train_data)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max([len(doc) for doc in train_data if doc is not None])\n",
        "\n",
        "x_train = encode_docs(tokenizer, max_length, x_train)\n",
        "x_test = encode_docs(tokenizer, max_length, x_test)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "# Creating model\n",
        "model = cnn_model(x_train, y_train, tokenizer, max_length)\n",
        "model.fit(x_train, y_train, batch_size=100, epochs=50, verbose=1, callbacks=None,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "# Tokenizing data\n",
        "train_data = get_token()['data']\n",
        "label = get_token()['label']\n",
        "train_label = np.array([create_label(n) for n in label])\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.3)\n",
        "tokenizer = create_tokenizer(train_data)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max([len(doc) for doc in train_data if doc is not None])\n",
        "\n",
        "x_train = encode_docs(tokenizer, max_length, x_train)\n",
        "x_test = encode_docs(tokenizer, max_length, x_test)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "#Creating model\n",
        "model = cnn_model(x_train, y_train, tokenizer, maxlength)\n",
        "model.fit(x_train, y_train, batch_size=100, epochs=50, verbose=1, callbacks=None,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "###################################\n",
        "#         Classification          #\n",
        "###################################\n",
        "\n",
        "###################################\n",
        "#                                 #\n",
        "#          scikit-learn           #\n",
        "#                                 #\n",
        "###################################"
      ],
      "metadata": {
        "id": "FzJo69rqjVEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Решение задачи регрессии при помощи scikit-learn**"
      ],
      "metadata": {
        "id": "B34JIRs6nLW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "#                                 #\n",
        "#          scikit-learn           #\n",
        "#                                 #\n",
        "###################################\n",
        "\n",
        "###################################\n",
        "#           Regression            #\n",
        "###################################\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generating synthetic data\n",
        "X = np.random.rand(1000, 10) # 1000 samples, 10 features\n",
        "Y = X @ np.random.rand(10, 1) + np.random.rand(1000, 1) # Linear\n",
        "# combination with noise\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Initializing the MLPRegressor\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
        "\n",
        "# Training the model\n",
        "mlp.fit(X_train, Y_train.ravel())\n",
        "\n",
        "# Making predictions\n",
        "Y_pred = mlp.predict(X_test)\n",
        "\n",
        "# Calculating the mean squared error\n",
        "mse = mean_squared_error(Y_test, Y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(mlp)\n",
        "\n",
        "# Prepare the dataset for the regression\n",
        "\n",
        "###################################\n",
        "#           Regression            #\n",
        "###################################\n",
        "\n",
        "###################################\n",
        "#                                 #\n",
        "#          scikit-learn           #\n",
        "#                                 #\n",
        "###################################"
      ],
      "metadata": {
        "id": "4ZqDOvejjau7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c3cf525-ffb3-4149-8705-fac08e0f30f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.0905711752751663\n",
            "MLPRegressor(max_iter=1000, random_state=42)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Заключение, выводы**"
      ],
      "metadata": {
        "id": "zNyx5d47nYaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7dCZWd4AuViL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pipreqs"
      ],
      "metadata": {
        "id": "ZzsPxfw_wYj8",
        "outputId": "55f1e2f6-13bf-4c94-f2f0-e06e779a5ece",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pipreqs\n",
            "  Downloading pipreqs-0.5.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting docopt==0.6.2 (from pipreqs)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(-1, 16 * 5 * 5)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "class RegressionModel(nn.Module):\n",
        "  def __init__(self, input_size):\n",
        "    super(RegressionModel, self).__init__()\n",
        "    self.hidden = nn.Linear(input_size, 64)\n",
        "    self.output = nn.Linear(64, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = th.relu(self.hidden(x))\n",
        "    x = self.output(x)\n",
        "    return x\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "net = Net()\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n",
        "\n",
        "for _ in range(10000): # Number of epochs\n",
        "  input = th.randn(1)\n",
        "  print(input)\n",
        "  output = net(input)\n",
        "  loss = th.abs(output)\n",
        "  net.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "net.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PlTCiC2G3Iyn",
        "outputId": "bb7e8b92-4985-4232-ad95-3927d8f316c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.0101])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mabs(output)\n\u001b[1;32m     44\u001b[0m net\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[24], line 15\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 15\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     16\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     17\u001b[0m   x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1]"
          ]
        }
      ]
    }
  ]
}